__author__ = 'Michal Kosinski'import jsonimport urllibimport codecsimport requestsURL = 'http://monitoring.krakow.pios.gov.pl/dane-pomiarowe/pobierz'HEADERS = {    'Accept'           : 'application/json, text/javascript, */*; q=0.01',    'Accept-Encoding'  : 'gzip, deflate',    'Accept-Language'  : 'pl-PL,pl;q=0.8,en-US;q=0.6,en;q=0.4',    'Connection'       : 'keep-alive',    'Content-Type'     : 'application/x-www-form-urlencoded; charset=UTF-8',    'Host'             : 'monitoring.krakow.pios.gov.pl',    'Origin'           : 'http://monitoring.krakow.pios.gov.pl',    'Referer'          : 'http://monitoring.krakow.pios.gov.pl/dane-pomiarowe/automatyczne/stacja/5/parametry/31-36-34-30-32/dzienny/16.05.2015',    'User-Agent'       : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',    'X-Requested-With' : 'XMLHttpRequest'}query = {    "measType"         : "Auto",    "viewType"         : "Station",    "dateRange"        : "Day",    "date"             : "17.05.2015",    "viewTypeEntityId" : "5",    "channels"         : [31,36,34,30,32]}stations = {    4: 'Tarnow',    5: 'Skawina',    6: 'Aleja Krasinskiego',    7: 'Nowa Huta',    8: 'Nowy Sacz',    9: 'Zakopane',    10: 'Olkusz',    11: 'Trzebinia',    12: 'Szymbark',    15: 'Szarow',    16: 'Krakow-Kurdwanow',    19: 'Limanowa P',    20: 'Szczawnica P',    21: 'Kety P',    23: 'Slomniki P',    24: 'Myslenice P',    25: 'Bukowno P',    29: 'Sucha Beskidzka II',    148: 'Szarow Spokojna'}channels = { 'pl_PL': {                47: 'Tlenki Azotu',                49: 'Benzen',                50: 'Tlenek Wegla',                53: 'Tlenek Azotu',                54: 'Dwutlenek Azotu',                57: 'PM10',                61: 'Dwutlenek Siarki',                146: 'Ozon',                211: 'PM25' },            'en_US': {                47: 'Nitrogen Monoxides',                49: 'Benzene',                50: 'Carbon Monoxide',                53: 'Nitrogen Monoxide',                54: 'Nitrogen Dioxide',                57: 'PM10',                61: 'Sulfur Dioxide',                146: 'Ozone',                211: 'PM25'                }}def getRawData(query):    """    Encodes query to web page, sends request and retrieves data    :param query: dict of query parameters    :return: raw JSON response from page    """    encoded_query = "query=" + urllib.quote(json.dumps(query, separators=(",",":")))    result = requests.post(URL, headers = HEADERS, data = encoded_query)    #print "headers: {}\nstatus: {}\n{}".format(r.headers, r.status_code, r.text)    return result.contentskip = ('avg', 'data', 'thresholds', 'thresholdsForAvg')labels = ('timestamp', 'value', 'aggType', 'chartTooltipContent', 'count', 'coverageRate', 'decimals', 'extStartTime', 'interval',          'isAvgValid', 'label', 'ord', 'paramCode', 'paramId', 'paramLabel', 'paramPostfix', 'retroCount',          'scaleMax', 'scaleMin', 'startTime', 'unit', 'unitLabel')def blank(size):    return [None] * sizedef index_of(label):    return labels.index(label)def flatten(json_data):    #pprint(json.loads(data))    data = json_data    storage = []    for ser in data['series']:        print codecs.encode(ser['paramLabel'], 'utf-8')        row = blank(len(labels))        for label in labels:            if label not in ser.keys():                print "Warning: key \'{}\' not found!"            else:                row[index_of(label)] = ser[label]        # deal with nested data        # if ser['avg'] is not None:        #     for key, val in ser['avg'].iteritems():        #         row['avg.'+key] = val        # if ser['thresholds'] is not None:        #     for key, val in ser['thresholds'].iteritems():        #         row['thresholds.'+key] = val        # if ser['thresholdsForAvg'] is not None:        #     for key, val in ser['thresholdsForAvg'].iteritems():        #         row['thresholdsForAvg.'+key] = val        # deal with measurements        for sample in ser['data']:            new = list(row)            new[index_of('timestamp')], new[index_of('value')] = sample            storage.append(new)            print(len(new))    return storageif __name__ == '__main__':    from pprint import pprint    from samples import SAMPLE_RESPONSE    storage = flatten(SAMPLE_RESPONSE['data'])    pprint(storage)    #pprint(json.loads(getRawData(query)))    #if SAMPLE_RESPONSE['success']:    #    print "\n".join([str(x) for x in flatten(SAMPLE_RESPONSE['data'])])    #else:    #    print "Data retrieval failed"