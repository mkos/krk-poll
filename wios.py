__author__ = 'Michal Kosinski'import jsonimport urllibimport codecsimport requestsURL = 'http://monitoring.krakow.pios.gov.pl/dane-pomiarowe/pobierz'HEADERS = {    'Accept'           : 'application/json, text/javascript, */*; q=0.01',    'Accept-Encoding'  : 'gzip, deflate',    'Accept-Language'  : 'pl-PL,pl;q=0.8,en-US;q=0.6,en;q=0.4',    'Connection'       : 'keep-alive',    'Content-Type'     : 'application/x-www-form-urlencoded; charset=UTF-8',    'Host'             : 'monitoring.krakow.pios.gov.pl',    'Origin'           : 'http://monitoring.krakow.pios.gov.pl',    'Referer'          : 'http://monitoring.krakow.pios.gov.pl/dane-pomiarowe/automatyczne/stacja/5/parametry/31-36-34-30-32/dzienny/16.05.2015',    'User-Agent'       : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',    'X-Requested-With' : 'XMLHttpRequest'}query = {    "measType"         : "Auto",    "viewType"         : "Station",    "dateRange"        : "Day",    "date"             : "17.05.2015",    "viewTypeEntityId" : "5",    "channels"         : [31,36,34,30,32]}stations = {    4: 'Tarnow',    5: 'Skawina',    6: 'Aleja Krasinskiego',    7: 'Nowa Huta',    8: 'Nowy Sacz',    9: 'Zakopane',    10: 'Olkusz',    11: 'Trzebinia',    12: 'Szymbark',    15: 'Szarow',    16: 'Krakow-Kurdwanow',    19: 'Limanowa P',    20: 'Szczawnica P',    21: 'Kęty P',    23: 'Slomniki P',    24: 'Myslenice P',    25: 'Bukowno P'    29: 'Sucha Beskidzka II',    148: 'Szarow Spokojna'}channels = {    47: 'Tlenki Azotu',    49: 'Benzen',    50: 'Tlenek Węgla',    54: 'Dwutlenek Azotu',    53: 'Tlenek Azotu',    57: 'PM10',    61: 'Dwutlenek Siarki',    146: 'Ozon',    211: 'PM25'}def getRawData(query):    """    Encodes query to web page, sends request and retrieves data    :param query: dict of query parameters    :return: raw JSON response from page    """    encoded_query = "query=" + urllib.quote(json.dumps(query, separators=(",",":")))    result = requests.post(URL, headers = HEADERS, data = encoded_query)    #print "headers: {}\nstatus: {}\n{}".format(r.headers, r.status_code, r.text)    return result.contentdef flatten(data):    #pprint(json.loads(data))    storage = []    omitted = ('avg', 'data', 'thresholds', 'thresholdsForAvg')    for ser in data['series']:        print codecs.encode(ser['paramLabel'], 'utf-8')        row = { k: v for k, v in ser.iteritems() if k not in omitted}        # deal with nested data        if ser['avg'] is not None:            for key, val in ser['avg'].iteritems():                row['avg.'+key] = val        if ser['thresholds'] is not None:            for key, val in ser['thresholds'].iteritems():                row['thresholds.'+key] = val        if ser['thresholdsForAvg'] is not None:            for key, val in ser['thresholdsForAvg'].iteritems():                row['thresholdsForAvg.'+key] = val        # deal with measurements        for (timestamp, meas) in ser['data']:            row_new = row.copy()            row_new['timestamp'] = timestamp            row_new['measurement'] = meas            row_list = [row_new[k] for k in sorted(row_new, key=row_new.get)]            storage.append(row_list)            print(len(row_list))    return storageif __name__ == '__main__':    from pprint import pprint    pprint(json.loads(getRawData(query)))    #if SAMPLE_RESPONSE['success']:    #    print "\n".join([str(x) for x in flatten(SAMPLE_RESPONSE['data'])])    #else:    #    print "Data retrieval failed"